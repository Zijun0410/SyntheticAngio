{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch, math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change the relevant elements in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fcos_resnet50_fpn(\n",
    "    weights=torchvision.models.detection.FCOS_ResNet50_FPN_Weights.DEFAULT,\n",
    "    weights_backbone=torchvision.models.ResNet50_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.backbone.out_channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_anchors = model.head.classification_head.num_anchors\n",
    "num_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 2\n",
    "updated_cls_logits = torch.nn.Conv2d(model.backbone.out_channels, num_anchors*num_class, kernel_size=3, stride=1, padding=1)\n",
    "torch.nn.init.normal_(updated_cls_logits.weight, std=0.01)  # as per pytorch code\n",
    "torch.nn.init.constant_(updated_cls_logits.bias, -math.log((1 - 0.01) / 0.01)) \n",
    "model.head.classification_head.cls_logits = updated_cls_logits\n",
    "model.head.classification_head.num_classes = num_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(256, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.head.classification_head.cls_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transform = torchvision.models.detection.transform.GeneralizedRCNNTransform(min_size=512, max_size=512, image_mean=[0.485, 0.456, 0.406], image_std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On how to change the number of classes in the model\n",
    "\n",
    "[https://discuss.pytorch.org/t/object-detection-fine-tuning-model-initialisation-error/159940/4](Link)\n",
    "\n",
    "\n",
    "```python\n",
    "from torchvision.models.detection import fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights\n",
    "import math\n",
    "weights = FCOS_ResNet50_FPN_Weights.DEFAULT\n",
    "model = fcos_resnet50_fpn(weights=weights)  # load an object detection model pre-trained on COCO\n",
    "num_anchors = model.head.classification_head.num_anchors\n",
    "model.head.classification_head.num_classes = num_class\n",
    "out_channels = model.head.classification_head.conv[9].out_channels\n",
    "cls_logits = torch.nn.Conv2d(out_channels, num_anchors * num_class, kernel_size=3, stride=1, padding=1)\n",
    "torch.nn.init.normal_(cls_logits.weight, std=0.01)\n",
    "torch.nn.init.constant_(cls_logits.bias, -math.log((1 - 0.01) / 0.01))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCOS(\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelP6P7(\n",
       "        (p6): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (anchor_generator): AnchorGenerator()\n",
       "  (head): FCOSHead(\n",
       "    (classification_head): FCOSClassificationHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (5): ReLU()\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (8): ReLU()\n",
       "        (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (11): ReLU()\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (regression_head): FCOSRegressionHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (5): ReLU()\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (8): ReLU()\n",
       "        (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (11): ReLU()\n",
       "      )\n",
       "      (bbox_reg): Conv2d(256, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bbox_ctrness): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(512,), max_size=512, mode='bilinear')\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FCOS'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model).__name__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import SyntheticImage\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "umr_dir=Path(r'Z:\\Projects\\Angiogram\\Data\\Processed\\Zijun\\Synthetic\\Sythetic_Output\\UoMR')\n",
    "ukr_dir=Path(r'Z:\\Projects\\Angiogram\\Data\\Processed\\Zijun\\Synthetic\\Sythetic_Output\\UKR')\n",
    "dir_list = [umr_dir, ukr_dir]\n",
    "synthetic_image = SyntheticImage(dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "target, image = synthetic_image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model([image])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model in evaluation mode, the output is a list of dictionaries, one for each input image. The dictionary contains the following fields:\n",
    "`'boxes', 'scores','labels'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>),\n",
       "  'scores': tensor([], grad_fn=<IndexBackward0>),\n",
       "  'labels': tensor([], dtype=torch.int64)}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_threshold = 0.3\n",
    "STENOSIS_CLASSES = {1: 'mild', 2: 'severe'}\n",
    "pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "# Get all the predicted bounding boxes.\n",
    "pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n",
    "# Get boxes above the threshold score.\n",
    "boxes = pred_bboxes[pred_scores >= detection_threshold].astype(np.int32)\n",
    "labels = outputs[0]['labels'][pred_scores >= detection_threshold]\n",
    "# Get all the predicited class names.\n",
    "pred_classes = [STENOSIS_CLASSES[i] for i in labels.cpu().numpy()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model in training mode, the output is a dict containing multiple loss with the fowllowing keys:\n",
    "`'classification', 'bbox_regression', 'bbox_ctrness'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as module_dataloader\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutomized_collate(batch):\n",
    "    elem = batch[0]\n",
    "    \n",
    "    elem_type = type(elem)\n",
    "    print(elem_type)\n",
    "    if isinstance(elem, torch.Tensor):\n",
    "        out = None\n",
    "        if torch.utils.data.get_worker_info() is not None:\n",
    "            # If we're in a background process, concatenate directly into a\n",
    "            # shared memory tensor to avoid an extra copy\n",
    "            numel = sum([x.numel() for x in batch])\n",
    "            storage = elem.storage()._new_shared(numel)\n",
    "            out = elem.new(storage)\n",
    "        return torch.stack(batch, 0, out=out)\n",
    "\n",
    "    elif isinstance(elem, float):\n",
    "        return torch.tensor(batch, dtype=torch.float64)\n",
    "    elif isinstance(elem, int):\n",
    "        return torch.tensor(batch)\n",
    "    elif isinstance(elem, collections.abc.Mapping):\n",
    "        print('hello')\n",
    "        return list(batch)\n",
    "    elif isinstance(elem, tuple):  # namedtuple\n",
    "        print(batch)\n",
    "        return elem_type(*(cutomized_collate(samples) for samples in zip(*batch)))\n",
    "    elif isinstance(elem, collections.abc.Sequence):\n",
    "        # check to make sure that the elements in batch have consistent size\n",
    "        it = iter(batch)\n",
    "        elem_size = len(next(it))\n",
    "        if not all(len(elem) == elem_size for elem in it):\n",
    "            raise RuntimeError('each element in list of batch should be of equal size')\n",
    "        transposed = zip(*batch)\n",
    "        return [module_dataloader.dataloader.default_collate(samples) for samples in transposed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {'batch_size': 4,\n",
    "    'collate_fn':cutomized_collate,\n",
    "    'sampler':module_dataloader.sampler.RandomSampler(range(len(synthetic_image))),\n",
    "    'drop_last':True,\n",
    "    'shuffle':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_image_dataloader = torch.utils.data.DataLoader(synthetic_image, **variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "[({'boxes': tensor([[264.5000, 416.5000, 282.5000, 434.5000]]), 'labels': tensor([0])}, tensor([[[0.4627, 0.4706, 0.4902,  ..., 0.4157, 0.3804, 0.3608],\n",
      "         [0.4667, 0.4745, 0.4824,  ..., 0.4157, 0.3961, 0.3765],\n",
      "         [0.4745, 0.4745, 0.4863,  ..., 0.4314, 0.4078, 0.3922],\n",
      "         ...,\n",
      "         [0.2000, 0.2039, 0.1961,  ..., 0.1765, 0.1804, 0.1843],\n",
      "         [0.2039, 0.2039, 0.1961,  ..., 0.1765, 0.1765, 0.1804],\n",
      "         [0.1922, 0.1961, 0.1961,  ..., 0.1725, 0.1725, 0.1725]],\n",
      "\n",
      "        [[0.4627, 0.4706, 0.4902,  ..., 0.4157, 0.3804, 0.3608],\n",
      "         [0.4667, 0.4745, 0.4824,  ..., 0.4157, 0.3961, 0.3765],\n",
      "         [0.4745, 0.4745, 0.4863,  ..., 0.4314, 0.4078, 0.3922],\n",
      "         ...,\n",
      "         [0.2000, 0.2039, 0.1961,  ..., 0.1765, 0.1804, 0.1843],\n",
      "         [0.2039, 0.2039, 0.1961,  ..., 0.1765, 0.1765, 0.1804],\n",
      "         [0.1922, 0.1961, 0.1961,  ..., 0.1725, 0.1725, 0.1725]],\n",
      "\n",
      "        [[0.4627, 0.4706, 0.4902,  ..., 0.4157, 0.3804, 0.3608],\n",
      "         [0.4667, 0.4745, 0.4824,  ..., 0.4157, 0.3961, 0.3765],\n",
      "         [0.4745, 0.4745, 0.4863,  ..., 0.4314, 0.4078, 0.3922],\n",
      "         ...,\n",
      "         [0.2000, 0.2039, 0.1961,  ..., 0.1765, 0.1804, 0.1843],\n",
      "         [0.2039, 0.2039, 0.1961,  ..., 0.1765, 0.1765, 0.1804],\n",
      "         [0.1922, 0.1961, 0.1961,  ..., 0.1725, 0.1725, 0.1725]]])), ({'boxes': tensor([[109.0000, 341.5000, 127.0000, 359.5000]]), 'labels': tensor([1])}, tensor([[[0.2353, 0.2392, 0.2510,  ..., 0.1176, 0.1216, 0.1176],\n",
      "         [0.2392, 0.2431, 0.2549,  ..., 0.1216, 0.1176, 0.1176],\n",
      "         [0.2471, 0.2588, 0.2549,  ..., 0.1216, 0.1216, 0.1255],\n",
      "         ...,\n",
      "         [0.1451, 0.1569, 0.1529,  ..., 0.1137, 0.1176, 0.1098],\n",
      "         [0.1412, 0.1412, 0.1490,  ..., 0.1137, 0.1098, 0.1098],\n",
      "         [0.1373, 0.1333, 0.1333,  ..., 0.1137, 0.1098, 0.1059]],\n",
      "\n",
      "        [[0.2353, 0.2392, 0.2510,  ..., 0.1176, 0.1216, 0.1176],\n",
      "         [0.2392, 0.2431, 0.2549,  ..., 0.1216, 0.1176, 0.1176],\n",
      "         [0.2471, 0.2588, 0.2549,  ..., 0.1216, 0.1216, 0.1255],\n",
      "         ...,\n",
      "         [0.1451, 0.1569, 0.1529,  ..., 0.1137, 0.1176, 0.1098],\n",
      "         [0.1412, 0.1412, 0.1490,  ..., 0.1137, 0.1098, 0.1098],\n",
      "         [0.1373, 0.1333, 0.1333,  ..., 0.1137, 0.1098, 0.1059]],\n",
      "\n",
      "        [[0.2353, 0.2392, 0.2510,  ..., 0.1176, 0.1216, 0.1176],\n",
      "         [0.2392, 0.2431, 0.2549,  ..., 0.1216, 0.1176, 0.1176],\n",
      "         [0.2471, 0.2588, 0.2549,  ..., 0.1216, 0.1216, 0.1255],\n",
      "         ...,\n",
      "         [0.1451, 0.1569, 0.1529,  ..., 0.1137, 0.1176, 0.1098],\n",
      "         [0.1412, 0.1412, 0.1490,  ..., 0.1137, 0.1098, 0.1098],\n",
      "         [0.1373, 0.1333, 0.1333,  ..., 0.1137, 0.1098, 0.1059]]])), ({'boxes': tensor([[176.5000, 372.0000, 196.5000, 392.0000]]), 'labels': tensor([1])}, tensor([[[0.1529, 0.1569, 0.1569,  ..., 0.1608, 0.1333, 0.1255],\n",
      "         [0.1490, 0.1608, 0.1569,  ..., 0.1725, 0.1451, 0.1451],\n",
      "         [0.1490, 0.1451, 0.1608,  ..., 0.1765, 0.1490, 0.1373],\n",
      "         ...,\n",
      "         [0.1412, 0.1333, 0.1373,  ..., 0.1216, 0.1176, 0.1137],\n",
      "         [0.1373, 0.1373, 0.1412,  ..., 0.1176, 0.1098, 0.1176],\n",
      "         [0.1373, 0.1373, 0.1333,  ..., 0.1216, 0.1137, 0.1255]],\n",
      "\n",
      "        [[0.1529, 0.1569, 0.1569,  ..., 0.1608, 0.1333, 0.1255],\n",
      "         [0.1490, 0.1608, 0.1569,  ..., 0.1725, 0.1451, 0.1451],\n",
      "         [0.1490, 0.1451, 0.1608,  ..., 0.1765, 0.1490, 0.1373],\n",
      "         ...,\n",
      "         [0.1412, 0.1333, 0.1373,  ..., 0.1216, 0.1176, 0.1137],\n",
      "         [0.1373, 0.1373, 0.1412,  ..., 0.1176, 0.1098, 0.1176],\n",
      "         [0.1373, 0.1373, 0.1333,  ..., 0.1216, 0.1137, 0.1255]],\n",
      "\n",
      "        [[0.1529, 0.1569, 0.1569,  ..., 0.1608, 0.1333, 0.1255],\n",
      "         [0.1490, 0.1608, 0.1569,  ..., 0.1725, 0.1451, 0.1451],\n",
      "         [0.1490, 0.1451, 0.1608,  ..., 0.1765, 0.1490, 0.1373],\n",
      "         ...,\n",
      "         [0.1412, 0.1333, 0.1373,  ..., 0.1216, 0.1176, 0.1137],\n",
      "         [0.1373, 0.1373, 0.1412,  ..., 0.1176, 0.1098, 0.1176],\n",
      "         [0.1373, 0.1373, 0.1333,  ..., 0.1216, 0.1137, 0.1255]]])), ({'boxes': tensor([[210.0000, 315.5000, 228.0000, 333.5000]]), 'labels': tensor([0])}, tensor([[[0.2235, 0.2392, 0.2431,  ..., 0.2471, 0.1686, 0.1255],\n",
      "         [0.2235, 0.2431, 0.2549,  ..., 0.2549, 0.1686, 0.1255],\n",
      "         [0.2275, 0.2431, 0.2510,  ..., 0.2706, 0.1725, 0.1255],\n",
      "         ...,\n",
      "         [0.0980, 0.1098, 0.1098,  ..., 0.1255, 0.1294, 0.1255],\n",
      "         [0.1020, 0.1059, 0.1098,  ..., 0.1255, 0.1216, 0.1255],\n",
      "         [0.1020, 0.1059, 0.1059,  ..., 0.1255, 0.1255, 0.1255]],\n",
      "\n",
      "        [[0.2235, 0.2392, 0.2431,  ..., 0.2471, 0.1686, 0.1255],\n",
      "         [0.2235, 0.2431, 0.2549,  ..., 0.2549, 0.1686, 0.1255],\n",
      "         [0.2275, 0.2431, 0.2510,  ..., 0.2706, 0.1725, 0.1255],\n",
      "         ...,\n",
      "         [0.0980, 0.1098, 0.1098,  ..., 0.1255, 0.1294, 0.1255],\n",
      "         [0.1020, 0.1059, 0.1098,  ..., 0.1255, 0.1216, 0.1255],\n",
      "         [0.1020, 0.1059, 0.1059,  ..., 0.1255, 0.1255, 0.1255]],\n",
      "\n",
      "        [[0.2235, 0.2392, 0.2431,  ..., 0.2471, 0.1686, 0.1255],\n",
      "         [0.2235, 0.2431, 0.2549,  ..., 0.2549, 0.1686, 0.1255],\n",
      "         [0.2275, 0.2431, 0.2510,  ..., 0.2706, 0.1725, 0.1255],\n",
      "         ...,\n",
      "         [0.0980, 0.1098, 0.1098,  ..., 0.1255, 0.1294, 0.1255],\n",
      "         [0.1020, 0.1059, 0.1098,  ..., 0.1255, 0.1216, 0.1255],\n",
      "         [0.1020, 0.1059, 0.1059,  ..., 0.1255, 0.1255, 0.1255]]]))]\n",
      "<class 'dict'>\n",
      "hello\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple expected at most 1 arguments, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10524\\4266244252.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreal_image_dataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zijung\\.conda\\envs\\zijung\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zijung\\.conda\\envs\\zijung\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zijung\\.conda\\envs\\zijung\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10524\\2434161813.py\u001b[0m in \u001b[0;36mcutomized_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# namedtuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcutomized_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# check to make sure that the elements in batch have consistent size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple expected at most 1 arguments, got 2"
     ]
    }
   ],
   "source": [
    "for targets, images in real_image_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[[187.5000, 363.0000, 205.5000, 381.0000]],\n",
       " \n",
       "         [[261.0000, 222.0000, 281.0000, 242.0000]],\n",
       " \n",
       "         [[332.5000, 313.5000, 350.5000, 331.5000]],\n",
       " \n",
       "         [[156.0000, 329.0000, 174.0000, 347.0000]]]),\n",
       " 'labels': tensor([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]])}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_list = list(list(targets[key]) for key in targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict_list = [dict(zip(targets.keys(), [eg_list[i][j] for i in range(len(targets))])) for j in range(len(eg_list[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[187.5000, 363.0000, 205.5000, 381.0000]]),\n",
       "  'labels': tensor([1])},\n",
       " {'boxes': tensor([[261., 222., 281., 242.]]), 'labels': tensor([1])},\n",
       " {'boxes': tensor([[332.5000, 313.5000, 350.5000, 331.5000]]),\n",
       "  'labels': tensor([1])},\n",
       " {'boxes': tensor([[156., 329., 174., 347.]]), 'labels': tensor([1])}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [{k: v.to('cpu') for k, v in t.items()} for t in target_dict_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[187.5000, 363.0000, 205.5000, 381.0000]]),\n",
       "  'labels': tensor([1])},\n",
       " {'boxes': tensor([[261., 222., 281., 242.]]), 'labels': tensor([1])},\n",
       " {'boxes': tensor([[332.5000, 313.5000, 350.5000, 331.5000]]),\n",
       "  'labels': tensor([1])},\n",
       " {'boxes': tensor([[156., 329., 174., 347.]]), 'labels': tensor([1])}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "with torch.no_grad():\n",
    "    outputs = model([image]*4, target_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[249.5000, 412.5000, 269.5000, 432.5000]]),\n",
       " 'labels': tensor([1])}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classification': tensor(1.1089),\n",
       " 'bbox_regression': tensor(0.9021),\n",
       " 'bbox_ctrness': tensor(0.8343)}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = {'boxes': torch.tensor([[[264.5000, 430.0000, 284.5000, 450.0000]],\n",
    "        [[117.0000, 245.0000, 135.0000, 263.0000]]]), 'labels': torch.tensor([[0],\n",
    "        [0]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 'l'), ('o', 'a'), ('x', 'b'), ('e', 'e'), ('s', 'l')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(*targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classification': tensor(1.0974),\n",
       " 'bbox_regression': tensor(0.8437),\n",
       " 'bbox_ctrness': tensor(0.7195)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[249.5000, 412.5000, 269.5000, 432.5000]]),\n",
       "  'labels': tensor([1])}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classification': tensor(1.1333, grad_fn=<DivBackward0>),\n",
       " 'bbox_regression': tensor(0.8437, grad_fn=<DivBackward0>),\n",
       " 'bbox_ctrness': tensor(0.7195, grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = sum(loss for loss in outputs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6914, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the number of examples in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import SyntheticImage\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "umr_dir=Path(r'Z:\\Projects\\Angiogram\\Data\\Processed\\Zijun\\Synthetic\\Sythetic_Output\\UoMR')\n",
    "ukr_dir=Path(r'Z:\\Projects\\Angiogram\\Data\\Processed\\Zijun\\Synthetic\\Sythetic_Output\\UKR')\n",
    "synthetic_image_umr = SyntheticImage([umr_dir])\n",
    "synthetic_image_ukr = SyntheticImage([ukr_dir])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1357"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(synthetic_image_umr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(synthetic_image_ukr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Real Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import RealImage\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "umr_dir = Path(r'Z:\\Projects\\Angiogram\\Data\\Processed\\Zijun\\Synthetic\\Real_Image\\UMR\\Full')\n",
    "ukr_dir = Path(r'Z:\\Projects\\Angiogram\\Data\\Processed\\Zijun\\Synthetic\\Real_Image\\UKR\\Full')\n",
    "dir_list = [umr_dir, ukr_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_image_data = RealImage(dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_image_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zijung",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b81dc32a35ae386ea1f16667cb01eb4ca83d360de83d4f9c09f949e11b490c26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
