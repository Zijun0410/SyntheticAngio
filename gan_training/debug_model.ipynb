{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Check the Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(object):\n",
    "    \"\"\"ResNet model with modified output layer\"\"\"\n",
    "    def __init__(self, input_channel=1, output_dim=2):\n",
    "        super().__init__()\n",
    "        # extract fc layers features\n",
    "        self.model = models.resnet18()\n",
    "        num_features = self.model.fc.in_features     \n",
    "        self.model.fc = nn.Linear(num_features, output_dim)\n",
    "        if input_channel !=3:\n",
    "            self.model.conv1 = nn.Conv2d(input_channel, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) \n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = ResNet18().get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imput_tensor = torch.Tensor(5, 1, 512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = discriminator(imput_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0225, -0.0084],\n",
       "        [ 0.0225, -0.0084],\n",
       "        [ 0.0225, -0.0084],\n",
       "        [ 0.0225, -0.0084],\n",
       "        [ 0.0225, -0.0084]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Check the Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, depth=4):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.depth = depth\n",
    "\n",
    "        init_n = 64\n",
    "        self.inc = DoubleConv(n_channels, init_n)\n",
    "\n",
    "        for iDep in range(depth):\n",
    "            setattr(self, f'down{iDep+1}', \n",
    "                Down(init_n*2**iDep, init_n*2**(iDep+1)))\n",
    "            # e.g. when the depth is 4, the following 'down' layers would be creates \n",
    "                # self.down1 = Down(64, 128)\n",
    "                # self.down2 = Down(128, 256)\n",
    "                # self.down3 = Down(256, 512)\n",
    "                # self.down4 = Down(512, 1024)\n",
    "            setattr(self, f'up{iDep+1}', \n",
    "                Up(init_n*2**(depth-iDep), init_n*2**(depth-iDep-1)))\n",
    "            # e.g. when the depth is 4, the following 'up' layers would be creates \n",
    "                # self.up1 = Up(1024, 512)\n",
    "                # self.up2 = Up(512, 256)\n",
    "                # self.up3 = Up(256, 128)\n",
    "                # self.up4 = Up(128, 64)\n",
    "\n",
    "        self.outc = OutConv(init_n, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        depth = self.depth\n",
    "        setattr(self, 'x0', self.inc(x)) # self.x0 = self.inc(x)\n",
    "        for iDep in range(depth):\n",
    "            setattr(self, f'x{iDep+1}', \n",
    "                getattr(self, f'down{iDep+1}')(getattr(self, f'x{iDep}'))\n",
    "                )\n",
    "        # e.g. when the depth is 4, forward through the following 'down' layers  \n",
    "            # x1 = self.down1(x0)\n",
    "            # x2 = self.down2(x1)\n",
    "            # x3 = self.down3(x2)\n",
    "            # x4 = self.down4(x3)\n",
    "\n",
    "        for iDep in range(depth):\n",
    "            setattr(self, f'x{depth-iDep}', \n",
    "                getattr(self, f'up{iDep+1}')(\n",
    "                    getattr(self, f'x{depth-iDep}'), \n",
    "                    getattr(self, f'x{depth-iDep-1}')\n",
    "                    )\n",
    "                )\n",
    "        # e.g. when the depth is 4, forward through the following 'up' layers    \n",
    "            # x3 = self.up1(x4, x3)\n",
    "            # x2 = self.up2(x3, x2)\n",
    "            # x1 = self.up3(x2, x1)\n",
    "            # x0 = self.up4(x1, x0)\n",
    "\n",
    "        logits = self.outc(self.x0)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = UNet(2, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "imput_tensor = torch.Tensor(5, 2, 512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generator(imput_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 512, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0659, -0.0659, -0.0659,  ..., -0.0659, -0.0659, -0.0659],\n",
       "         [-0.0659, -0.0659, -0.0659,  ..., -0.0659, -0.0659, -0.0659],\n",
       "         [-0.0659, -0.0659, -0.0659,  ..., -0.0659, -0.0659, -0.0659],\n",
       "         ...,\n",
       "         [-0.0659, -0.0659, -0.0659,  ..., -0.0659, -0.0659, -0.0659],\n",
       "         [-0.0659, -0.0659, -0.0659,  ..., -0.0659, -0.0659, -0.0659],\n",
       "         [-0.0659, -0.0659, -0.0659,  ..., -0.0659, -0.0659, -0.0659]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1, :, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Parameter Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The total number of parameter is {total_param/1e6 :.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameter is 1.86\n"
     ]
    }
   ],
   "source": [
    "count_parameters(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11171266"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 ('digit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f61e0a8fe4dea7226de18c6503f360d9c71c7a34958c2f40ba60d7ca77fac524"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
